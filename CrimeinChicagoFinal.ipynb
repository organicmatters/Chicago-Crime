{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime in Chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to predict whether a person who committed a particular crime was arrested for the city of Chicago. The city of Chicago Data Portal has every crime dating back to 2001 in it's database with location and crime information for each crime.  This dataset will be combined with NOAA weather data and a model will be created for arrests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import feather\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sqlalchemy import create_engine  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump, load\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the warnings was included at the end for a cleaner looking notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I loaded the chicago crime data onto a remote server and pulled the data down as needed. Using postgreSQL, I created the schema for the data to be loaded onto the remote server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL Query for remote server:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE TABLE IF NOT EXISTS ChicagoCrime (  \n",
    "        ID integer,  \n",
    "        CaseNumber varchar(20),  \n",
    "        Date varchar(50),  \n",
    "        Block varchar(50),  \n",
    "        IUCR varchar(10),  \n",
    "        PrimaryType varchar(50),  \n",
    "        Description varchar(100),  \n",
    "        LocationDescription varchar(150),  \n",
    "        Arrest varchar(10),  \n",
    "        Domestic varchar(10),  \n",
    "        Beat integer,  \n",
    "        District real,  \n",
    "        Ward real,  \n",
    "        CommunityArea real,  \n",
    "        FBICode varchar(10),  \n",
    "        XCoordinate varchar(20),  \n",
    "        YCoordinate varchar(20),  \n",
    "        Year integer,  \n",
    "        UpdatedOn varchar(50),  \n",
    "        Latitude varchar(15),  \n",
    "        Longitude varchar(15),  \n",
    "        Location varchar(50)  \n",
    "    );  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Data From Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was pulled in from the remote server to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnx = create_engine('postgresql://ubuntu@ec2-100-24-40-180.compute-1.amazonaws.com/chicago')\n",
    "df = pd.read_sql_query('''SELECT * FROM chicagocrime''', cnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datetimes will need to be converted to datetime format for pandas to recognize the type as a date. I also went ahead and checked to see what the arrest rate was for all crimes committed in chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of Crimes ending in Arrest: 0.27685069987994154\n"
     ]
    }
   ],
   "source": [
    "df['datetime'] = pd.to_datetime(df['date'], infer_datetime_format=True)\n",
    "mask = df['arrest']  == 'true'\n",
    "print('Percent of Crimes ending in Arrest: ' + str(len(df[mask])/len(df))b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 27.7% of crimes in Chicago end in arrest. Yikes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with the data locally we will serialize the data with feather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.to_feather('chicago_crime.feather')\n",
    "df = feather.read_dataframe('chicago_crime.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather data from NOAA was pulled in as it may have some predictive ability for our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_data_path = '/Users/kevin/Downloads/1598904.csv'\n",
    "df_weather = pd.read_csv(weather_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather Column Descriptions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WT03 - Thunder  \n",
    "WT04 - Ice pellets, sleet, snow pellets, or small hail\"  \n",
    "PRCP - Precipitation  \n",
    "WT05 - Hail (may include small hail)  \n",
    "WV03 - Thunder  \n",
    "WT06 - Glaze or rime   \n",
    "WT07 - Dust, volcanic ash, blowing dust, blowing sand, or blowing obstruction  \n",
    "WT08 - Smoke or haze   \n",
    "SNWD - Snow depth  \n",
    "WT09 - Blowing or drifting snow  \n",
    "WDF2 - Direction of fastest 2-minute wind  \n",
    "WDF5 - Direction of fastest 5-second wind  \n",
    "PGTM - Peak gust time  \n",
    "WT11 - High or damaging winds  \n",
    "TMAX - Maximum temperature  \n",
    "WT13 - Mist  \n",
    "WSF2 - Fastest 2-minute wind speed  \n",
    "FMTM - Time of fastest mile or fastest 1-minute wind  \n",
    "WSF5 - Fastest 5-second wind speed  \n",
    "SNOW - Snowfall  \n",
    "WT14 - Drizzle  \n",
    "WT15 - Freezing drizzle   \n",
    "WT16 - Rain (may include freezing rain, drizzle, and freezing drizzle)\"   \n",
    "WT17 - Freezing rain   \n",
    "WT18 - Snow, snow pellets, snow grains, or ice crystals  \n",
    "WT19 - Unknown source of precipitation   \n",
    "AWND - Average wind speed  \n",
    "WT21 - Ground fog  \n",
    "WT22 - Ice fog or freezing fog  \n",
    "WV20 - Rain or snow shower  \n",
    "WT01 - Fog, ice fog, or freezing fog (may include heavy fog)  \n",
    "WESD - Water equivalent of snow on the ground  \n",
    "WT02 - Heavy fog or heaving freezing fog (not always distinguished from fog)  \n",
    "TAVG - Average Temperature.  \n",
    "TMIN - Minimum temperature  \n",
    "TSUN - Total sunshine for the period  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather data will need to be merged with the crime data by date so converted the weather dates to datetimes. I also made the columns lowercase for consistency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_weather.columns = map(str.lower, df_weather.columns)\n",
    "df_weather['datetime'] = pd.to_datetime(df_weather['date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Weather Data and Crime Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that both weather and crime data are loaded in I merged the two dataframes into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_weather = df_weather.sort_values('datetime')\n",
    "df = df.sort_values('datetime')\n",
    "cw_df = pd.merge_asof(df, df_weather, on = 'datetime', direction = 'backward', tolerance = pd.Timedelta('1 day')) \n",
    "df = cw_df.reset_index()\n",
    "#df.to_feather('chicago_crime_and_weather.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the combined weather and crime dataset I decided which columns to drop. Each column was analyzed and the decision to drop the column was made individually, hence why there are several repetitive drop methods.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('index', axis = 1)\n",
    "df = df.drop('casenumber', axis = 1)\n",
    "df = df.drop('id', axis = 1)\n",
    "df = df.drop('block', axis = 1)\n",
    "df = df.drop('station', axis = 1)\n",
    "df = df.drop('fmtm', axis = 1)\n",
    "df = df.drop('pgtm', axis = 1)\n",
    "df = df.drop('snwd', axis = 1)\n",
    "df = df.drop('xcoordinate', axis = 1)\n",
    "df = df.drop('ycoordinate', axis = 1)\n",
    "df = df.drop('datetime', axis = 1)\n",
    "df = df.drop('tavg', axis = 1)\n",
    "df = df.drop('date_y', axis = 1)\n",
    "df = df.drop('iucr', axis = 1)\n",
    "df = df.drop('name', axis = 1)\n",
    "df = df.drop('year', axis = 1)\n",
    "df = df.drop('updatedon', axis = 1)\n",
    "df = df.drop('location', axis = 1)\n",
    "df = df.drop('fbicode', axis = 1)\n",
    "df = df.drop('description', axis = 1)\n",
    "df = df.drop('date_x', axis = 1)\n",
    "df = df.drop(['wdf2', 'wdf5', 'wesd', 'wsf2', 'wsf5', 'wt01',\n",
    "       'wt02', 'wt03', 'wt04', 'wt05', 'wt06', 'wt07', 'wt08', 'wt09', 'wt11',\n",
    "       'wt13', 'wt14', 'wt15', 'wt16', 'wt17', 'wt18', 'wt19', 'wt21', 'wt22',\n",
    "       'wv03', 'wv20', 'tsun'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have all the data I want in my dataframe, I will convert all data types to the proper type so they can be fed into a sklearn classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['primarytype'] = df['primarytype'].astype('category')\n",
    "df['description'] = df['description'].astype('category')\n",
    "df['locationdescription'] = df['locationdescription'].astype('category')\n",
    "df['arrest'].replace('true', 1, inplace = True)\n",
    "df['arrest'].replace('false', 0, inplace = True)\n",
    "df['domestic'].replace('true', 1, inplace = True)\n",
    "df['domestic'].replace('false', 0, inplace = True)\n",
    "df['fbicode'] = df['fbicode'].astype('category')\n",
    "df['xcoordinate'] = df['xcoordinate'].fillna(value=np.nan)\n",
    "df['xcoordinate'] = df['xcoordinate'].astype('int64', errors = 'ignore')\n",
    "df['ycoordinate'] = df['ycoordinate'].fillna(value=np.nan)\n",
    "df['ycoordinate'] = df['ycoordinate'].astype('int64', errors = 'ignore')\n",
    "df['latitude'] = df['latitude'].astype('float64', errors = 'ignore')\n",
    "df['longitude'] = df['longitude'].astype('float64', errors = 'ignore')\n",
    "df['station'] = df['station'].astype('category')\n",
    "df = df.dropna(subset=['district'])\n",
    "df = df.dropna(subset=['latitude'])\n",
    "df['locationdescription'] = df.locationdescription.fillna(value='OTHER')\n",
    "df['communityarea'] = df.sort_values(by=['beat', 'district', 'ward'])['communityarea'].fillna(method='ffill')\n",
    "df['ward'] = df.sort_values(by=['beat', 'district', 'communityarea'])['ward'].fillna(method='ffill')\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is cleaned, I wanted to make sure there are no nulls in the dataset that would cause issues in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for header in df.columns:\n",
    "    \n",
    "    nulls_count = df[f'{header}'].isnull().sum()\n",
    "    \n",
    "    print(f'There are {nulls_count} in {header}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now cleaned and ready to be examined closer. The work done so far will be saved in a feather file for quick data loading in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_feather('chicago_crime_cleaned.feather')\n",
    "df = feather.read_dataframe('chicago_crime_cleaned.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_feather('chicago_crime_final.feather')\n",
    "df = feather.read_dataframe('chicago_crime_final.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first wanted to check for linear relationships in the data. I ran a pearson correlation to see if there were any relationships between the features and the label. I also wanted to check for colinearity between the features, which could lead to model overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>arrest</th>\n",
       "      <th>domestic</th>\n",
       "      <th>beat</th>\n",
       "      <th>district</th>\n",
       "      <th>ward</th>\n",
       "      <th>communityarea</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>awnd</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055044</td>\n",
       "      <td>0.043370</td>\n",
       "      <td>-0.035996</td>\n",
       "      <td>-0.004956</td>\n",
       "      <td>0.013127</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>-0.005265</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.036246</td>\n",
       "      <td>0.016684</td>\n",
       "      <td>0.020514</td>\n",
       "      <td>0.017340</td>\n",
       "      <td>0.038502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrest</th>\n",
       "      <td>-0.055044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.069274</td>\n",
       "      <td>-0.015993</td>\n",
       "      <td>-0.016780</td>\n",
       "      <td>-0.015836</td>\n",
       "      <td>-0.008292</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>-0.031477</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>-0.009167</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>-0.023662</td>\n",
       "      <td>-0.025416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domestic</th>\n",
       "      <td>0.043370</td>\n",
       "      <td>-0.069274</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.041821</td>\n",
       "      <td>-0.038657</td>\n",
       "      <td>-0.050101</td>\n",
       "      <td>0.072056</td>\n",
       "      <td>-0.075669</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.003772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beat</th>\n",
       "      <td>-0.035996</td>\n",
       "      <td>-0.015993</td>\n",
       "      <td>-0.041821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.939092</td>\n",
       "      <td>0.635785</td>\n",
       "      <td>-0.506381</td>\n",
       "      <td>0.612650</td>\n",
       "      <td>-0.473687</td>\n",
       "      <td>-0.003126</td>\n",
       "      <td>-0.000468</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>-0.002319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>district</th>\n",
       "      <td>-0.004956</td>\n",
       "      <td>-0.016780</td>\n",
       "      <td>-0.038657</td>\n",
       "      <td>0.939092</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.688740</td>\n",
       "      <td>-0.499337</td>\n",
       "      <td>0.620597</td>\n",
       "      <td>-0.528367</td>\n",
       "      <td>-0.001122</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>-0.001339</td>\n",
       "      <td>-0.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ward</th>\n",
       "      <td>0.013127</td>\n",
       "      <td>-0.015836</td>\n",
       "      <td>-0.050101</td>\n",
       "      <td>0.635785</td>\n",
       "      <td>0.688740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.532559</td>\n",
       "      <td>0.626385</td>\n",
       "      <td>-0.432463</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.000588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communityarea</th>\n",
       "      <td>0.004968</td>\n",
       "      <td>-0.008292</td>\n",
       "      <td>0.072056</td>\n",
       "      <td>-0.506381</td>\n",
       "      <td>-0.499337</td>\n",
       "      <td>-0.532559</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747118</td>\n",
       "      <td>0.240317</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>-0.000435</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.001377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>-0.005265</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>-0.075669</td>\n",
       "      <td>0.612650</td>\n",
       "      <td>0.620597</td>\n",
       "      <td>0.626385</td>\n",
       "      <td>-0.747118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.410834</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000483</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>-0.003313</td>\n",
       "      <td>-0.002630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>0.001056</td>\n",
       "      <td>-0.031477</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>-0.473687</td>\n",
       "      <td>-0.528367</td>\n",
       "      <td>-0.432463</td>\n",
       "      <td>0.240317</td>\n",
       "      <td>-0.410834</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002323</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>-0.002221</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>0.007829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awnd</th>\n",
       "      <td>0.036246</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>-0.003126</td>\n",
       "      <td>-0.001122</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.002323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.080271</td>\n",
       "      <td>0.099045</td>\n",
       "      <td>-0.250733</td>\n",
       "      <td>-0.215913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prcp</th>\n",
       "      <td>0.016684</td>\n",
       "      <td>-0.009167</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>-0.000468</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>-0.000483</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.080271</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104215</td>\n",
       "      <td>0.092846</td>\n",
       "      <td>0.142637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow</th>\n",
       "      <td>0.020514</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>-0.000435</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>-0.002221</td>\n",
       "      <td>0.099045</td>\n",
       "      <td>0.104215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.227082</td>\n",
       "      <td>-0.203525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tmax</th>\n",
       "      <td>0.017340</td>\n",
       "      <td>-0.023662</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>-0.001339</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>-0.003313</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>-0.250733</td>\n",
       "      <td>0.092846</td>\n",
       "      <td>-0.227082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tmin</th>\n",
       "      <td>0.038502</td>\n",
       "      <td>-0.025416</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>-0.002319</td>\n",
       "      <td>-0.001220</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>-0.002630</td>\n",
       "      <td>0.007829</td>\n",
       "      <td>-0.215913</td>\n",
       "      <td>0.142637</td>\n",
       "      <td>-0.203525</td>\n",
       "      <td>0.941816</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  index    arrest  domestic      beat  district      ward  \\\n",
       "index          1.000000 -0.055044  0.043370 -0.035996 -0.004956  0.013127   \n",
       "arrest        -0.055044  1.000000 -0.069274 -0.015993 -0.016780 -0.015836   \n",
       "domestic       0.043370 -0.069274  1.000000 -0.041821 -0.038657 -0.050101   \n",
       "beat          -0.035996 -0.015993 -0.041821  1.000000  0.939092  0.635785   \n",
       "district      -0.004956 -0.016780 -0.038657  0.939092  1.000000  0.688740   \n",
       "ward           0.013127 -0.015836 -0.050101  0.635785  0.688740  1.000000   \n",
       "communityarea  0.004968 -0.008292  0.072056 -0.506381 -0.499337 -0.532559   \n",
       "latitude      -0.005265  0.002096 -0.075669  0.612650  0.620597  0.626385   \n",
       "longitude      0.001056 -0.031477  0.004518 -0.473687 -0.528367 -0.432463   \n",
       "awnd           0.036246  0.001616  0.002332 -0.003126 -0.001122  0.000059   \n",
       "prcp           0.016684 -0.009167  0.002825 -0.000468 -0.000027 -0.000011   \n",
       "snow           0.020514  0.002330  0.002082  0.000737  0.000919  0.001221   \n",
       "tmax           0.017340 -0.023662  0.004467 -0.002075 -0.001339 -0.000049   \n",
       "tmin           0.038502 -0.025416  0.003772 -0.002319 -0.001220  0.000588   \n",
       "\n",
       "               communityarea  latitude  longitude      awnd      prcp  \\\n",
       "index               0.004968 -0.005265   0.001056  0.036246  0.016684   \n",
       "arrest             -0.008292  0.002096  -0.031477  0.001616 -0.009167   \n",
       "domestic            0.072056 -0.075669   0.004518  0.002332  0.002825   \n",
       "beat               -0.506381  0.612650  -0.473687 -0.003126 -0.000468   \n",
       "district           -0.499337  0.620597  -0.528367 -0.001122 -0.000027   \n",
       "ward               -0.532559  0.626385  -0.432463  0.000059 -0.000011   \n",
       "communityarea       1.000000 -0.747118   0.240317  0.000821  0.001185   \n",
       "latitude           -0.747118  1.000000  -0.410834 -0.000028 -0.000483   \n",
       "longitude           0.240317 -0.410834   1.000000 -0.002323  0.000345   \n",
       "awnd                0.000821 -0.000028  -0.002323  1.000000  0.080271   \n",
       "prcp                0.001185 -0.000483   0.000345  0.080271  1.000000   \n",
       "snow               -0.000435  0.001649  -0.002221  0.099045  0.104215   \n",
       "tmax                0.001802 -0.003313   0.007822 -0.250733  0.092846   \n",
       "tmin                0.001377 -0.002630   0.007829 -0.215913  0.142637   \n",
       "\n",
       "                   snow      tmax      tmin  \n",
       "index          0.020514  0.017340  0.038502  \n",
       "arrest         0.002330 -0.023662 -0.025416  \n",
       "domestic       0.002082  0.004467  0.003772  \n",
       "beat           0.000737 -0.002075 -0.002319  \n",
       "district       0.000919 -0.001339 -0.001220  \n",
       "ward           0.001221 -0.000049  0.000588  \n",
       "communityarea -0.000435  0.001802  0.001377  \n",
       "latitude       0.001649 -0.003313 -0.002630  \n",
       "longitude     -0.002221  0.007822  0.007829  \n",
       "awnd           0.099045 -0.250733 -0.215913  \n",
       "prcp           0.104215  0.092846  0.142637  \n",
       "snow           1.000000 -0.227082 -0.203525  \n",
       "tmax          -0.227082  1.000000  0.941816  \n",
       "tmin          -0.203525  0.941816  1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All categorical variables in the dataframe will need to be converted to dummies before they are loaded into the model.  After the dummies are added to the model, the original categorical columns can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['primarytype'])], axis = 1)\n",
    "df = df.drop('primarytype', axis = 1)\n",
    "df = pd.concat([df, pd.get_dummies(df['locationdescription'])], axis = 1)\n",
    "df = df.drop('locationdescription', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An evaluation function was created to consistently analyze all of the models to determine which model was the best.  The AUC score was the score that will tell me which model was the best performing.  The validation and testing accuracy scores will tell me how accurate the model was with out of sample data.  The training accuracy can be compared to the validation and testing accuracy and will let me know if the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(clf):\n",
    "    \n",
    "    train_preds = clf.predict(X_train)\n",
    "    train_auc = roc_auc_score(y_train, train_preds)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    val_auc = roc_auc_score(y_val, val_preds)\n",
    "    test_preds = clf.predict(X_test)\n",
    "    test_auc = roc_auc_score(y_test, test_preds)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    val_score = clf.score(X_val, y_val)\n",
    "    test_score = clf.score(X_test, y_test)\n",
    "    confusion_mat = confusion_matrix(y_test, test_preds)\n",
    "    \n",
    "    return print(f\"AUC for training set: {train_auc} \\nAUC for validation set: {val_auc} \\nAUC for test set: {test_auc} \\nScore for training set: {train_score}\\nScore for validation set: {val_score} \\nScore for test set: {test_score} \\nConfusion Matrix: \\n{confusion_mat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.to_feather('chicago_crime_model_data.feather')\n",
    "df = feather.read_dataframe('chicago_crime_model_data.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to drop several more variables which were somewhat redundant in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['index', 'domestic', 'beat', 'district', 'ward', 'communityarea'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features and labels were split before loading into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['arrest']\n",
    "X = df.drop('arrest', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the dataset (~6.7 million rows) I decided to use a single validation set while testing my model. Using cross validation is the most reliable way to create a model.  However, it is computationally expensive and would be time consuming to do so.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model I ran was a logistic regression model. This model required that the data was scaled before it was added to the model. I used a sklearn pipeline to handle the scaling and running of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_logistic_pipeline = Pipeline([('scale_train', StandardScaler()),  ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logistic_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(clf_logistic_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC for training set: 0.7819176581804969  \n",
    "AUC for validation set: 0.7817091792999603   \n",
    "AUC for test set: 0.7818304472777092   \n",
    "Score for training set: 0.8653519925078904  \n",
    "Score for validation set: 0.8651978977998159  \n",
    "Score for test set: 0.8653141251789568  \n",
    "Confusion Matrix:   \n",
    "[[1415779   45369]  \n",
    " [ 226796  332795]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this trained model in the future, I saved the model using joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(clf_logistic_pipeline, 'clf_logistic_pipeline.joblib') \n",
    "#clf_logistic_pipeline = load('filename.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression allows for the betas to be analyzed. Each individual value of e^beta is the likelihood of an arrest of an individual crime as compared to all other crimes. I went ahead and looked at the top 10 crimes that had the highest probability of arrest as well as the bottom 10 crimes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_list = []\n",
    "\n",
    "\n",
    "for tup in zip(X_train.columns, np.exp(clf_logistic_pipeline.named_steps['lr'].coef_[0])):\n",
    "    \n",
    "    importance_list.append(tup) \n",
    "    sorted_importance_list = sorted(importance_list, key=lambda tup: tup[1], reverse = True)\n",
    "    sorted_importance_list_reversed = sorted(importance_list, key=lambda tup: tup[1], reverse = False)\n",
    "sorted_importance_list[0:10], sorted_importance_list_reversed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "([('NARCOTICS', 6.449520558275791),  \n",
    "  ('PROSTITUTION', 1.94090044673702),  \n",
    "  ('DEPARTMENT STORE', 1.3555689429902227),  \n",
    "  ('CRIMINAL TRESPASS', 1.3481573587031486),  \n",
    "  ('GAMBLING', 1.3177199360656247),  \n",
    "  ('GROCERY FOOD STORE', 1.3097096245166744),  \n",
    "  ('LIQUOR LAW VIOLATION', 1.2945964863184407),  \n",
    "  ('WEAPONS VIOLATION', 1.273162809482963),  \n",
    "  ('DRUG STORE', 1.1821135829524616),  \n",
    "  ('INTERFERENCE WITH PUBLIC OFFICER', 1.1708414586148312)],  \n",
    " [('THEFT', 0.509949611367234),  \n",
    "  ('CRIMINAL DAMAGE', 0.6028952015130741),  \n",
    "  ('BURGLARY', 0.6737965347995939),  \n",
    "  ('ROBBERY', 0.7442151529506927),  \n",
    "  ('MOTOR VEHICLE THEFT', 0.762481021645242),  \n",
    "  ('RESIDENCE', 0.8164103236220748),  \n",
    "  ('DECEPTIVE PRACTICE', 0.8252460436674496),  \n",
    "  ('RESIDENCE-GARAGE', 0.907916971075862),  \n",
    "  ('OTHER OFFENSE', 0.9155231101266241),  \n",
    "  ('BATTERY', 0.9170271875463851)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model that I chose was a random forest. This model was chosen since it usually a good predictive model. In many cases the model is too good of a predictor on the training set and overfits the data.  The forest usually needs some pruning to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators = 50, max_depth = 10, min_samples_leaf = 2, oob_score=True, n_jobs=-1)\n",
    "clf_rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(clf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC for training set: 0.7378734801893847  \n",
    "AUC for validation set: 0.7380699441781224   \n",
    "AUC for test set: 0.7381047408102378  \n",
    "Score for training set: 0.8523847165569017  \n",
    "Score for validation set: 0.8523207198494469   \n",
    "Score for test set: 0.8525074242640934   \n",
    "Confusion Matrix:   \n",
    "[[1453149    7999]  \n",
    " [ 290045  269546]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests have a somewhat built in validation set called the out-of-bag score that is good to check to ensure the model is not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84806602800330488"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests also have an important attribute called the feature importance score.  Random forest classifiers split trees by the purity of the data so the feature importance is the number of times a feature shows up in a tree while weighting features higher that show up closer to the root node of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NARCOTICS', 0.52285329801007996),\n",
       " ('CRIMINAL TRESPASS', 0.071447538463131677),\n",
       " ('THEFT', 0.06582852637974039),\n",
       " ('PROSTITUTION', 0.050390058821393059),\n",
       " ('SIDEWALK', 0.044579003890294074),\n",
       " ('CRIMINAL DAMAGE', 0.032731838878306781),\n",
       " ('WEAPONS VIOLATION', 0.030913909633955301),\n",
       " ('DEPARTMENT STORE', 0.017539930207047666),\n",
       " ('RESIDENCE', 0.017514975629403056),\n",
       " ('BURGLARY', 0.016004138556858317)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You should create function for this and put it in a class\n",
    "importance_list = []\n",
    "\n",
    "for tup in zip(X_train.columns, clf_rf.feature_importances_):\n",
    "    \n",
    "    importance_list.append(tup) \n",
    "    sorted_importance_list = sorted(importance_list, key=lambda tup: tup[1], reverse = True)\n",
    "sorted_importance_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Narcotics is the most important feature. Since most narcotics arrests are the result of raids, it can be inferred intuitively that it would be an important feature in determining arrests.  Similar intuition can be used for theft.  Theft is common and most of the time there are no arrests for theft so if the crime is theft, there is not a high likelihood of arrest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf_rf.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf_rf, 'clf_rf.joblib') \n",
    "#clf_rd = load('filename.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to see if the hyperparameters for the random forest model could be further optimized.  I used a grid searh over several models to see if the hyperparameters could be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "parameters = {'n_estimators':[10,20,30], 'max_depth' : [3,7, 10, None], 'min_samples_leaf':[1,3,5,7]}\n",
    "rfc_clf = GridSearchCV(rfc, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rfc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters were called for the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_clf.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV(cv=5, error_score='raise',  \n",
    "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',  \n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,  \n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,  \n",
    "            min_samples_leaf=1, min_samples_split=2,  \n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,  \n",
    "            oob_score=False, random_state=None, verbose=0,  \n",
    "            warm_start=False),  \n",
    "       fit_params=None, iid=True, n_jobs=1,  \n",
    "       param_grid={'n_estimators': [10, 20, 30], 'max_depth': [3, 7, 10, None], 'min_samples_leaf': [1, 3, 5, 7]},  \n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',  \n",
    "       scoring=None, verbose=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfb = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model that I tried was K nearest neighbors.  This model is the slowest to run since the euclidean distance needs to be calculated for every point to every other point so it is computationally expensive.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_knn_pipeline = Pipeline([('scale_train', StandardScaler()),  ('lr', KNeighborsClassifier(n_neighbors=5, n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scale_train', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lr', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_knn_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for training set: 0.8304842129492409 \n",
      "AUC for validation set: 0.7900891238840703 \n",
      "AUC for test set: 0.7887500002362207 \n",
      "Score for training set: 0.8863387755102041\n",
      "Score for validation set: 0.8525904761904762 \n",
      "Score for test set: 0.8523133333333334 \n",
      "Confusion Matrix: \n",
      "[[199987  12419]\n",
      " [ 31887  55707]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(clf_knn_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(clf_knn_pipeline, 'knn_clf_pipeline.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbrs = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 58s, sys: 1.53 s, total: 18min 59s\n",
      "Wall time: 18min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nbrs.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(nbrs, X_train=X_train_scaled, X_val=X_val_scaled, X_test=X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81331357275038685"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% time\n",
    "train_preds = nbrs.predict(X_train)\n",
    "roc_auc_score(y_train, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76138047840754519"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test score\n",
    "# n_neighbors=5 scores 0.7659\n",
    "preds = nbrs.predict(X_val)\n",
    "roc_auc_score(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[139939,   8366],\n",
       "       [ 25963,  35732]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mext model that I chose was gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier(learning_rate=0.01)\n",
    "gb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for training set: 0.7363564703752371 \n",
      "AUC for validation set: 0.7366049415361069 \n",
      "AUC for test set: 0.7366194163182307 \n",
      "Score for training set: 0.8500505523491769\n",
      "Score for validation set: 0.8500096852779533 \n",
      "Score for test set: 0.8501983680227877 \n",
      "Confusion Matrix: \n",
      "[[1448281   12867]\n",
      " [ 289843  269748]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(gb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only 27.7% of crimes ended up with an arrest, I decided to balance the dataset and try running models on the balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693394, 693394)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yay, balanced classes!\n",
    "len(y_resampled), len(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfb_balanced = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfb_balanced.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79100646692502496"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = rfb_balanced.predict(X_val)\n",
    "roc_auc_score(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.849547619047619"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfb_balanced.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84939666666666669"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfb_balanced.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[138361,   9944],\n",
       "       [ 21651,  40044]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of Several Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then wanted to try to put several of the models together in an ensemble and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [('lr', clf_logistic_pipeline), ('rf', clf_rf), ('gb', gb_clf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', Pipeline(memory=None,\n",
       "     steps=[('scale_train', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, ...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=-1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create voting classifier\n",
    "voting_classifer = VotingClassifier(estimators=model_list,\n",
    "                                    voting='hard', #<-- sklearn calls this hard voting\n",
    "                                    n_jobs=-1)\n",
    "voting_classifer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for training set: 0.7437367358291241 \n",
      "AUC for validation set: 0.7436991551717599 \n",
      "AUC for test set: 0.743873117622716 \n",
      "Score for training set: 0.8540023311343996\n",
      "Score for validation set: 0.8538194635911314 \n",
      "Score for test set: 0.8540613112331676 \n",
      "Confusion Matrix: \n",
      "[[1447775   13373]\n",
      " [ 281531  278060]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(voting_classifer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the logistic regression model performed the best.  These models could all be further tweaked for better performance, however, model with 88% accuracy predicting arrests for the city of Chicago is a good model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
